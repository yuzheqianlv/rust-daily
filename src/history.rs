use anyhow::Result;
use chrono::{DateTime, Utc, Duration};
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use std::fs;
use std::path::PathBuf;
use tracing::{debug, info, warn};

use crate::NewsItem;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedItem {
    pub id: String,           // æ–°é—»å”¯ä¸€æ ‡è¯† (URL hash)
    pub title: String,        // æ–°é—»æ ‡é¢˜
    pub url: String,          // æ–°é—»é“¾æ¥
    pub processed_at: DateTime<Utc>, // å¤„ç†æ—¶é—´
    pub source: String,       // æ¥æº
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct ProcessingHistory {
    pub items: Vec<ProcessedItem>,
    pub last_cleanup: Option<DateTime<Utc>>,
}

pub struct HistoryManager {
    history_file: PathBuf,
    history: ProcessingHistory,
}

impl HistoryManager {
    pub fn new() -> Result<Self> {
        let history_file = Self::get_history_file_path();
        
        // ç¡®ä¿ç›®å½•å­˜åœ¨
        if let Some(parent) = history_file.parent() {
            fs::create_dir_all(parent)?;
        }
        
        let history = Self::load_history(&history_file)?;
        
        Ok(Self {
            history_file,
            history,
        })
    }
    
    fn get_history_file_path() -> PathBuf {
        // ä½¿ç”¨ç”¨æˆ·ç›®å½•æˆ–å½“å‰ç›®å½•ä¸‹çš„ .rust-daily æ–‡ä»¶å¤¹
        let base_dir = dirs::home_dir()
            .unwrap_or_else(|| std::env::current_dir().unwrap_or_default())
            .join(".rust-daily");
        
        base_dir.join("processing_history.json")
    }
    
    fn load_history(path: &PathBuf) -> Result<ProcessingHistory> {
        if path.exists() {
            let content = fs::read_to_string(path)?;
            let history: ProcessingHistory = serde_json::from_str(&content)
                .unwrap_or_default();
            info!("åŠ è½½å†å²è®°å½•ï¼ŒåŒ…å« {} æ¡å·²å¤„ç†é¡¹ç›®", history.items.len());
            Ok(history)
        } else {
            info!("å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„è®°å½•");
            Ok(ProcessingHistory::default())
        }
    }
    
    fn save_history(&self) -> Result<()> {
        let content = serde_json::to_string_pretty(&self.history)?;
        fs::write(&self.history_file, content)?;
        debug!("ä¿å­˜å†å²è®°å½•åˆ°: {:?}", self.history_file);
        Ok(())
    }
    
    /// ç”Ÿæˆæ–°é—»é¡¹çš„å”¯ä¸€æ ‡è¯†
    fn generate_item_id(item: &NewsItem) -> String {
        use std::hash::{Hash, Hasher};
        use std::collections::hash_map::DefaultHasher;
        
        let mut hasher = DefaultHasher::new();
        item.link.hash(&mut hasher);
        item.title.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }
    
    /// è¿‡æ»¤æ‰å·²å¤„ç†çš„æ–°é—»é¡¹
    pub fn filter_unprocessed(&self, items: Vec<NewsItem>) -> Vec<NewsItem> {
        let processed_ids: HashSet<String> = self.history.items
            .iter()
            .map(|item| item.id.clone())
            .collect();
        
        let original_count = items.len();
        let filtered_items: Vec<NewsItem> = items
            .into_iter()
            .filter(|item| {
                let id = Self::generate_item_id(item);
                !processed_ids.contains(&id)
            })
            .collect();
        
        let filtered_count = filtered_items.len();
        let duplicate_count = original_count - filtered_count;
        
        if duplicate_count > 0 {
            info!("è¿‡æ»¤æ‰ {} æ¡å·²å¤„ç†çš„é‡å¤æ–°é—»ï¼Œå‰©ä½™ {} æ¡æ–°æ–°é—»", 
                  duplicate_count, filtered_count);
        } else {
            info!("æ²¡æœ‰å‘ç°é‡å¤æ–°é—»ï¼Œå…± {} æ¡æ–°æ–°é—»", filtered_count);
        }
        
        filtered_items
    }
    
    /// æ ‡è®°æ–°é—»é¡¹ä¸ºå·²å¤„ç†
    pub fn mark_as_processed(&mut self, items: &[NewsItem]) -> Result<()> {
        let now = Utc::now();
        
        for item in items {
            let processed_item = ProcessedItem {
                id: Self::generate_item_id(item),
                title: item.title.clone(),
                url: item.link.clone(),
                processed_at: now,
                source: item.source.clone(),
            };
            
            self.history.items.push(processed_item);
        }
        
        info!("æ ‡è®° {} æ¡æ–°é—»ä¸ºå·²å¤„ç†", items.len());
        self.save_history()?;
        Ok(())
    }
    
    /// æ¸…ç†è¿‡æœŸçš„å†å²è®°å½•
    pub fn cleanup_old_records(&mut self, days_to_keep: u64) -> Result<usize> {
        let cutoff_date = Utc::now() - Duration::days(days_to_keep as i64);
        let original_count = self.history.items.len();
        
        self.history.items.retain(|item| item.processed_at >= cutoff_date);
        
        let removed_count = original_count - self.history.items.len();
        
        if removed_count > 0 {
            self.history.last_cleanup = Some(Utc::now());
            self.save_history()?;
            info!("æ¸…ç†äº† {} æ¡è¶…è¿‡ {} å¤©çš„å†å²è®°å½•", removed_count, days_to_keep);
        }
        
        Ok(removed_count)
    }
    
    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> HistoryStats {
        let now = Utc::now();
        let today_start = now.date_naive().and_hms_opt(0, 0, 0).unwrap()
            .and_local_timezone(Utc).unwrap();
        let week_start = now - Duration::days(7);
        
        let today_count = self.history.items
            .iter()
            .filter(|item| item.processed_at >= today_start)
            .count();
        
        let week_count = self.history.items
            .iter()
            .filter(|item| item.processed_at >= week_start)
            .count();
        
        let sources: HashSet<String> = self.history.items
            .iter()
            .map(|item| item.source.clone())
            .collect();
        
        HistoryStats {
            total_processed: self.history.items.len(),
            today_processed: today_count,
            week_processed: week_count,
            unique_sources: sources.len(),
            last_cleanup: self.history.last_cleanup,
            oldest_record: self.history.items
                .iter()
                .map(|item| item.processed_at)
                .min(),
        }
    }
    
    /// æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•
    pub fn clear_all(&mut self) -> Result<()> {
        let count = self.history.items.len();
        self.history = ProcessingHistory::default();
        self.save_history()?;
        info!("æ¸…ç©ºäº†æ‰€æœ‰ {} æ¡å†å²è®°å½•", count);
        Ok(())
    }
    
    /// æœç´¢å†å²è®°å½•
    pub fn search(&self, query: &str) -> Vec<&ProcessedItem> {
        let query_lower = query.to_lowercase();
        self.history.items
            .iter()
            .filter(|item| {
                item.title.to_lowercase().contains(&query_lower) ||
                item.source.to_lowercase().contains(&query_lower) ||
                item.url.to_lowercase().contains(&query_lower)
            })
            .collect()
    }
}

#[derive(Debug)]
pub struct HistoryStats {
    pub total_processed: usize,
    pub today_processed: usize,
    pub week_processed: usize,
    pub unique_sources: usize,
    pub last_cleanup: Option<DateTime<Utc>>,
    pub oldest_record: Option<DateTime<Utc>>,
}

impl HistoryStats {
    pub fn display(&self) {
        println!("ğŸ“Š å†å²è®°å½•ç»Ÿè®¡:");
        println!("  æ€»å¤„ç†æ•°é‡: {}", self.total_processed);
        println!("  ä»Šæ—¥å¤„ç†: {}", self.today_processed);
        println!("  æœ¬å‘¨å¤„ç†: {}", self.week_processed);
        println!("  æ•°æ®æºæ•°é‡: {}", self.unique_sources);
        
        if let Some(cleanup) = self.last_cleanup {
            println!("  ä¸Šæ¬¡æ¸…ç†: {}", cleanup.format("%Y-%m-%d %H:%M:%S"));
        } else {
            println!("  ä¸Šæ¬¡æ¸…ç†: ä»æœªæ¸…ç†");
        }
        
        if let Some(oldest) = self.oldest_record {
            println!("  æœ€æ—©è®°å½•: {}", oldest.format("%Y-%m-%d %H:%M:%S"));
        }
    }
}